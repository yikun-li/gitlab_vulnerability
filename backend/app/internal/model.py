from typing import Tuple, Optional
import random
from datetime import datetime
from faker import Faker
from uuid import uuid4
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM
import torch

from app.config.app import settings
from app.database.schemas.gitlab import FileContent, SAMPLE_CVSS_VECTORS
from app.database.schemas import report_base as schemas
from app.internal.report import (
    create_scanner_schema,
    create_analyzer_schema,
    create_vulnerability_schema,
    create_remediation_schema,
)


class ModelManager:
    _instance = None
    detector_model = None
    detector_tokenizer = None
    general_model = None
    general_tokenizer = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelManager, cls).__new__(cls)
            # Initialize the model and tokenizer here
            cls._instance.load_model()
        return cls._instance

    def load_model(self):
        # load vulnerability detector model
        self.detector_tokenizer = AutoTokenizer.from_pretrained(settings.MODEL_PATH)
        self.detector_model = AutoModelForSequenceClassification.from_pretrained(
            settings.MODEL_PATH,
            device_map="auto",
        )
        self.detector_model.eval()

        # load vulnerability explanation model
        self.general_tokenizer = AutoTokenizer.from_pretrained(settings.MODEL_GENERAL)
        self.general_model = AutoModelForCausalLM.from_pretrained(
            settings.MODEL_GENERAL,
            device_map="auto",
        )
        self.general_model.eval()

    def analyze_code(self, code: str) -> str:
        with torch.no_grad():
            inputs = self.detector_tokenizer(code, return_tensors="pt", truncation=True, padding=True)
            # Move inputs to model's device
            inputs = {k: v.to(self.detector_model.device) for k, v in inputs.items()}

            outputs = self.detector_model(**inputs)
            predictions = outputs.logits.softmax(dim=-1)

            # Get the predicted label by finding the index of the maximum probability
            predicted_label = torch.argmax(predictions, dim=1).item()

            # Assign the label
            class_labels = ['Benign', 'Vulnerable']
            output_label = class_labels[predicted_label]

            print("Analyzing:", code)
            print("Prediction:", output_label)
            return output_label

    def explain_vulnerability(self, code: str) -> str:
        with torch.no_grad():
            # Create a prompt that asks the model to explain the vulnerability
            prompt = f"""Analyze the following code for security vulnerabilities and explain: 
            What specific vulnerability exists in the code

            Code to analyze:
            ```
            {code}
            ```

            **Answer in 50 words**:"""

            # Tokenize the prompt
            inputs = self.general_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True)
            # Move inputs to model's device
            inputs = {k: v.to(self.general_model.device) for k, v in inputs.items()}

            # Generate the explanation
            outputs = self.general_model.generate(
                **inputs,
                max_length=1024,
                num_return_sequences=1,
                temperature=0.1,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.general_tokenizer.eos_token_id
            )

            # Decode the generated explanation
            explanation = self.general_tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Remove the prompt from the explanation to get only the generated part
            explanation = explanation.replace(prompt, "").strip()
            return explanation

    def generate_title(self, code: str) -> str:
        with torch.no_grad():
            # Create a prompt that asks the model to generate title
            prompt = f"""Generate a short title showing the vulnerability in this code:
            ```
            {code}
            ```
            """

            # Tokenize the prompt
            inputs = self.general_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True)
            # Move inputs to model's device
            inputs = {k: v.to(self.general_model.device) for k, v in inputs.items()}

            # Generate the explanation
            outputs = self.general_model.generate(
                **inputs,
                max_length=1024,
                num_return_sequences=1,
                temperature=0.1,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.general_tokenizer.eos_token_id
            )

            # Decode the generated title
            title = self.general_tokenizer.decode(outputs[0], skip_special_tokens=True)

            # Remove the prompt from the title to get only the generated part
            title = title.replace(prompt, "").strip()
            return title


async def run(files: list[FileContent]) -> Tuple[
    bool, str, str, schemas.Scanner, schemas.Analyzer, list[schemas.Vulnerability], list[schemas.Remediation]]:
    start_time = datetime.now()

    scanner_metadata = create_scanner_schema(
        id=uuid4(),
        name="Security Vulnerability Detection LLM",
        version=":SKIP:",
        url="https://smu.edu.sg",
        vendor_name="SMU"
    )

    analyzer_metadata = create_analyzer_schema(
        id=uuid4(),
        name="SMU SCIS Vulnerability Analyzer",
        version=":SKIP:",
        url="https://smu.edu.sg",
        vendor_name="SMU"
    )

    success, vulnerabilities, remediations = await scan(files=files)
    end_time = datetime.now()
    return success, start_time, end_time, scanner_metadata, analyzer_metadata, vulnerabilities, remediations


async def mock_scan(files: list[FileContent]) -> Tuple[bool, list[schemas.Vulnerability], list[schemas.Remediation]]:
    '''mock_scan simulates what happens in an actual scan. This provides the actual implementation of the model scan a rough guideline of how to call variables and what variables are necessary.
    '''
    fake = Faker()
    vulnerabilities: list[schemas.Vulnerability] = []
    for f in files:
        type_value = f"{random.randint(50, 150)}"
        vulnerabilities.append(
            create_vulnerability_schema(
                type="cwe",
                type_value=type_value,
                type_url=f"https://cwe.mitre.org/data/definitions/{type_value}.html",
                name=fake.sentence(nb_words=3),
                description=fake.sentence(nb_words=10),
                severity=random.choice(list(schemas.SeverityLevels.__args__)),
                location_file=f.path,
                location_start_line=random.randint(1, 3),
                location_end_line=random.randint(4, 6),
                cvss_vector_vector=random.choice(SAMPLE_CVSS_VECTORS),
                cvss_vector_vendor_name=f"{uuid4()}"
            )
        )

    remediations: list[schemas.Remediation] = []
    vulnerability_ids = [v.id for v in vulnerabilities]
    for _ in vulnerabilities:
        if random.choice([True, False]):  # Randomly determine if there will be a remediation for the mock scan
            remediations.append(
                create_remediation_schema(
                    vulnerability_identifiers=random.choices(vulnerability_ids),
                    # Randomly select which vulnerabilities this remediation should tackle
                    summary=fake.sentence(nb_words=10),
                    diff=fake.sentence(nb_words=10),
                )
            )

    return True, vulnerabilities, remediations


async def scan(files: list[FileContent]) -> Tuple[bool, list[schemas.Vulnerability], list[schemas.Remediation]]:
    """Actual scan implementation using the loaded model"""
    model_manager = ModelManager()  # This will return the singleton instance
    vulnerabilities: list[schemas.Vulnerability] = []
    remediations: list[schemas.Remediation] = []

    for file in files:
        # Ensure code is a string
        code = file.content
        if not isinstance(code, str):
            try:
                # If it's bytes, decode it to string
                if isinstance(code, bytes):
                    code = code.decode('utf-8')
                else:
                    code = str(code)
            except Exception as e:
                raise ValueError(f"Could not convert code content to string: {e}")

        # Analyze each file using the loaded model
        analysis_result = model_manager.analyze_code(code)

        # Process the analysis results and create vulnerability entries
        if analysis_result == 'Vulnerable':
            # Explain vulnerability
            vul_explanation = model_manager.explain_vulnerability(code)
            vul_title = model_manager.generate_title(code)

            # Convert model predictions to vulnerability entries
            vuln = create_vulnerability_schema(
                type="cwe",
                type_value="test",
                type_url=f"https://cwe.mitre.org/data/definitions/111.html",
                name=vul_title,
                description=vul_explanation,
                severity=random.choice(list(schemas.SeverityLevels.__args__)),
                location_file=file.path,
                location_start_line=1,
                location_end_line=len(code.splitlines()),
                cvss_vector_vector=random.choice(SAMPLE_CVSS_VECTORS),
                cvss_vector_vendor_name=f"{uuid4()}"
            )
            vulnerabilities.append(vuln)

            # Optionally create remediation suggestions
            remediation = create_remediation_schema(
                vulnerability_identifiers=[vuln.id],
                summary="Suggested fix",  # Based on model output
                diff="Proposed changes"  # Based on model output
            )
            remediations.append(remediation)

    return True, vulnerabilities, remediations
