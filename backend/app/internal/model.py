from typing import Tuple, Optional
import random
from datetime import datetime
from faker import Faker
from uuid import uuid4
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

from app.config.app import settings
from app.database.schemas.gitlab import FileContent, SAMPLE_CVSS_VECTORS
from app.database.schemas import report_base as schemas
from app.internal.report import (
    create_scanner_schema,
    create_analyzer_schema,
    create_vulnerability_schema,
    create_remediation_schema,
)


class ModelManager:
    _instance = None
    model = None
    tokenizer = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelManager, cls).__new__(cls)
            # Initialize the model and tokenizer here
            cls._instance.load_model()
        return cls._instance

    def load_model(self):
        # load model
        self.tokenizer = AutoTokenizer.from_pretrained(settings.MODEL_PATH)
        self.model = AutoModelForSequenceClassification.from_pretrained(settings.MODEL_PATH, device_map="auto")
        self.model.eval()

    def analyze_code(self, code: str) -> str:
        # Implement your analysis logic here
        with torch.no_grad():
            # Ensure code is a string
            if not isinstance(code, str):
                try:
                    # If it's bytes, decode it to string
                    if isinstance(code, bytes):
                        code = code.decode('utf-8')
                    else:
                        code = str(code)
                except Exception as e:
                    raise ValueError(f"Could not convert code content to string: {e}")

            inputs = self.tokenizer(code, return_tensors="pt", truncation=True, padding=True)
            # Move inputs to model's device
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            outputs = self.model(**inputs)
            predictions = outputs.logits.softmax(dim=-1)

            # Get the predicted label by finding the index of the maximum probability
            predicted_label = torch.argmax(predictions, dim=1).item()

            # Assign the label
            class_labels = ['Vulnerable', 'Benign']
            output_label = class_labels[predicted_label]

            print("Analyzing:", code)
            print("Predictions:", output_label)
            return output_label


async def run(files: list[FileContent]) -> Tuple[
    bool, str, str, schemas.Scanner, schemas.Analyzer, list[schemas.Vulnerability], list[schemas.Remediation]]:
    start_time = datetime.now()

    # TODO: metadata should change according to the models we use internally
    scanner_metadata = create_scanner_schema(
        id=uuid4(),
        name="Security Vulnerability Detection LLM",
        version=":SKIP:",
        url="https://smu.edu.sg",
        vendor_name="SMU"
    )

    analyzer_metadata = create_analyzer_schema(
        id=uuid4(),
        name="SMU SCIS Vulnerability Analyzer",
        version=":SKIP:",
        url="https://smu.edu.sg",
        vendor_name="SMU"
    )

    success, vulnerabilities, remediations = await scan(files=files)
    end_time = datetime.now()
    return success, start_time, end_time, scanner_metadata, analyzer_metadata, vulnerabilities, remediations


async def mock_scan(files: list[FileContent]) -> Tuple[bool, list[schemas.Vulnerability], list[schemas.Remediation]]:
    '''mock_scan simulates what happens in an actual scan. This provides the actual implementation of the model scan a rough guideline of how to call variables and what variables are necessary.
    '''
    fake = Faker()
    vulnerabilities: list[schemas.Vulnerability] = []
    for f in files:
        type_value = f"{random.randint(50, 150)}"
        vulnerabilities.append(
            create_vulnerability_schema(
                type="cwe",
                type_value=type_value,
                type_url=f"https://cwe.mitre.org/data/definitions/{type_value}.html",
                name=fake.sentence(nb_words=3),
                description=fake.sentence(nb_words=10),
                severity=random.choice(list(schemas.SeverityLevels.__args__)),
                location_file=f.path,
                location_start_line=random.randint(1, 3),
                location_end_line=random.randint(4, 6),
                cvss_vector_vector=random.choice(SAMPLE_CVSS_VECTORS),
                cvss_vector_vendor_name=f"{uuid4()}"
            )
        )

    remediations: list[schemas.Remediation] = []
    vulnerability_ids = [v.id for v in vulnerabilities]
    for _ in vulnerabilities:
        if random.choice([True, False]):  # Randomly determine if there will be a remediation for the mock scan
            remediations.append(
                create_remediation_schema(
                    vulnerability_identifiers=random.choices(vulnerability_ids),
                    # Randomly select which vulnerabilities this remediation should tackle
                    summary=fake.sentence(nb_words=10),
                    diff=fake.sentence(nb_words=10),
                )
            )

    return True, vulnerabilities, remediations


async def scan(files: list[FileContent]) -> Tuple[bool, list[schemas.Vulnerability], list[schemas.Remediation]]:
    """Actual scan implementation using the loaded model"""
    model_manager = ModelManager()  # This will return the singleton instance
    vulnerabilities: list[schemas.Vulnerability] = []
    remediations: list[schemas.Remediation] = []

    for file in files:
        # Analyze each file using the loaded model
        analysis_result = model_manager.analyze_code(file.content)

        # Process the analysis results and create vulnerability entries
        if analysis_result == 'Vulnerable':
            # Convert model predictions to vulnerability entries
            vuln = create_vulnerability_schema(
                type="cwe",
                type_value="XXX",  # Replace with actual CWE from model prediction
                type_url=f"https://cwe.mitre.org/data/definitions/xxx.html",
                name="Detected Vulnerability",  # Replace with actual vulnerability name
                description="Vulnerability description",  # Replace with actual description
                severity="HIGH",  # Define based on model output
                location_file=file.path,
                location_start_line=1,  # Define based on model output
                location_end_line=2,  # Define based on model output
                cvss_vector_vector=SAMPLE_CVSS_VECTORS[0],  # Define based on model output
                cvss_vector_vendor_name="LLM-Scanner"
            )
            vulnerabilities.append(vuln)

            # Optionally create remediation suggestions
            remediation = create_remediation_schema(
                vulnerability_identifiers=[vuln.id],
                summary="Suggested fix",  # Based on model output
                diff="Proposed changes"  # Based on model output
            )
            remediations.append(remediation)

    return True, vulnerabilities, remediations
